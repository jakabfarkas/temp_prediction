{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3218 temperature data\n"
     ]
    }
   ],
   "source": [
    "temperatures = [] # emptz list for the temperatures\n",
    "count = 0\n",
    "with open('bud_temp.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        if row[0] != 'date': # do not read the first row\n",
    "            temperatures.append((float(row[1])+float(row[2]))/2) # load the average temeratures for each daz since 2012\n",
    "            count += 1\n",
    "print('Loaded',count,'temperature data') # data count\n",
    "temperatures = np.asarray(temperatures)\n",
    "temp_avg = np.average(temperatures)\n",
    "temp_mean = np.mean(temperatures)\n",
    "temperatures = (temperatures - temp_avg) / temp_mean # normalize the data for our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [] # empty arrays for the input and output data\n",
    "y1_data = []\n",
    "y2_data = []\n",
    "y3_data = []\n",
    "for i in range(count-365-28-1):\n",
    "    x_data.append(temperatures[i:i+365]) # the input of the neural network is the average temperature of the last 365 days\n",
    "    y1_data.append(temperatures[i+365+1]) # the output of the first network is the average temperature of the very next day after the last input\n",
    "    y2_data.append(temperatures[i+365+7]) # the output of the second network is the average temperature of the day after 7 days from the last input\n",
    "    y3_data.append(temperatures[i+365+28]) # the output of the third network is the average temperature of the day after 28 days from the last input\n",
    "x_data = np.asarray(x_data)\n",
    "y1_data = np.asarray(y1_data)\n",
    "y2_data = np.asarray(y2_data)\n",
    "y3_data = np.asarray(y3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # create the model\n",
    "model.add(Dense(100, input_shape=(x_data.shape[1],))) # the first hidden layer is a 100 neuron flat layer with sigmoid activation\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10)) # the first hidden layer is a 10 neuron flat layer with sigmoid activation\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1))# the output layer is a single nouron with tanh activation\n",
    "model.add(Activation('tanh'))\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.1, momentum=0.001, nesterov=True)) # compile the network\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss') # create the early stopping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to train and test arrays for each model\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x_data, y1_data, test_size=0.2, shuffle=True)\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x_data, y2_data, test_size=0.2, shuffle=True)\n",
    "x3_train, x3_test, y3_train, y3_test = train_test_split(x_data, y3_data, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2259 samples, validate on 565 samples\n",
      "Epoch 1/500\n",
      "2259/2259 [==============================] - 1s 376us/sample - loss: 0.1585 - val_loss: 0.0975\n",
      "Epoch 2/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0993 - val_loss: 0.0944\n",
      "Epoch 3/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0964 - val_loss: 0.0900\n",
      "Epoch 4/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0932 - val_loss: 0.0880\n",
      "Epoch 5/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0903 - val_loss: 0.0855\n",
      "Epoch 6/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0878 - val_loss: 0.0859\n",
      "Epoch 7/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0856 - val_loss: 0.0829\n",
      "Epoch 8/500\n",
      "2259/2259 [==============================] - 0s 56us/sample - loss: 0.0833 - val_loss: 0.0811\n",
      "Epoch 9/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0814 - val_loss: 0.0783\n",
      "Epoch 10/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0803 - val_loss: 0.0786\n",
      "Epoch 11/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0783 - val_loss: 0.0848\n",
      "Epoch 12/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0774 - val_loss: 0.0801\n",
      "Epoch 13/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0763 - val_loss: 0.0758\n",
      "Epoch 14/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0751 - val_loss: 0.0767\n",
      "Epoch 15/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0741 - val_loss: 0.0726\n",
      "Epoch 16/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0732 - val_loss: 0.0741\n",
      "Epoch 17/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0719 - val_loss: 0.0741\n",
      "Epoch 18/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0715 - val_loss: 0.0713\n",
      "Epoch 19/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0707 - val_loss: 0.0719\n",
      "Epoch 20/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0703 - val_loss: 0.0935\n",
      "Epoch 21/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0697 - val_loss: 0.0712\n",
      "Epoch 22/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0685 - val_loss: 0.0698\n",
      "Epoch 23/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0682 - val_loss: 0.0735\n",
      "Epoch 24/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0675 - val_loss: 0.0678\n",
      "Epoch 25/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0674 - val_loss: 0.0729\n",
      "Epoch 26/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0667 - val_loss: 0.0735\n",
      "Epoch 27/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0663 - val_loss: 0.0674\n",
      "Epoch 28/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0659 - val_loss: 0.0680\n",
      "Epoch 29/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0659 - val_loss: 0.0674\n",
      "Epoch 30/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0647 - val_loss: 0.0657\n",
      "Epoch 31/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0647 - val_loss: 0.0678\n",
      "Epoch 32/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0644 - val_loss: 0.0679\n",
      "Epoch 33/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0637 - val_loss: 0.0661\n",
      "Epoch 34/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0634 - val_loss: 0.0671\n",
      "Epoch 35/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0636 - val_loss: 0.0690\n",
      "Epoch 36/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0631 - val_loss: 0.0642\n",
      "Epoch 37/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0623 - val_loss: 0.0726\n",
      "Epoch 38/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0622 - val_loss: 0.0644\n",
      "Epoch 39/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0620 - val_loss: 0.0658\n",
      "Epoch 40/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0614 - val_loss: 0.0719\n",
      "Epoch 41/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0614 - val_loss: 0.0640\n",
      "Epoch 42/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0609 - val_loss: 0.0673\n",
      "Epoch 43/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0612 - val_loss: 0.0666\n",
      "Epoch 44/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0605 - val_loss: 0.0673\n",
      "Epoch 45/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0606 - val_loss: 0.0666\n",
      "Epoch 46/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0600 - val_loss: 0.0632\n",
      "Epoch 47/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0590 - val_loss: 0.0625\n",
      "Epoch 48/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0600 - val_loss: 0.0665\n",
      "Epoch 49/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0593 - val_loss: 0.0615\n",
      "Epoch 50/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0588 - val_loss: 0.0941\n",
      "Epoch 51/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0588 - val_loss: 0.0615\n",
      "Epoch 52/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0586 - val_loss: 0.0751\n",
      "Epoch 53/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0587 - val_loss: 0.0722\n",
      "Epoch 54/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0577 - val_loss: 0.0729\n",
      "Epoch 55/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0580 - val_loss: 0.0617\n",
      "Epoch 56/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0573 - val_loss: 0.0765\n",
      "Epoch 57/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0578 - val_loss: 0.0663\n",
      "Epoch 58/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0566 - val_loss: 0.0616\n",
      "Epoch 59/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0565 - val_loss: 0.0646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a56d9b3f48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the first model (prediction for the next day)\n",
    "model.fit(x1_train, y1_train,\n",
    "          batch_size=50,\n",
    "          epochs=500,\n",
    "          validation_data=(x1_test, y1_test),\n",
    "          callbacks=[early_stopping],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = np.empty([1,365])\n",
    "x_input[0] = temperatures[-365:] #the input data for the temperature predictions is the average temperature of the last 365 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.659762803262957\n"
     ]
    }
   ],
   "source": [
    "print('Temperature prediction for oct 28.:')\n",
    "print(model.predict(x_input)[0][0]*temp_mean+temp_avg) # predict the output and the normalized data back to a celsius value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2259 samples, validate on 565 samples\n",
      "Epoch 1/500\n",
      "2259/2259 [==============================] - 0s 64us/sample - loss: 0.0884 - val_loss: 0.0835\n",
      "Epoch 2/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0847 - val_loss: 0.0923\n",
      "Epoch 3/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0828 - val_loss: 0.0776\n",
      "Epoch 4/500\n",
      "2259/2259 [==============================] - 0s 56us/sample - loss: 0.0808 - val_loss: 0.0743\n",
      "Epoch 5/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0802 - val_loss: 0.0769\n",
      "Epoch 6/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0786 - val_loss: 0.0732\n",
      "Epoch 7/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0777 - val_loss: 0.0851\n",
      "Epoch 8/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0776 - val_loss: 0.0735\n",
      "Epoch 9/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0774 - val_loss: 0.0732\n",
      "Epoch 10/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0761 - val_loss: 0.0718\n",
      "Epoch 11/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0757 - val_loss: 0.0713\n",
      "Epoch 12/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0753 - val_loss: 0.0985\n",
      "Epoch 13/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0767 - val_loss: 0.0706\n",
      "Epoch 14/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0753 - val_loss: 0.0742\n",
      "Epoch 15/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0749 - val_loss: 0.0700\n",
      "Epoch 16/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0748 - val_loss: 0.0703\n",
      "Epoch 17/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0741 - val_loss: 0.0694\n",
      "Epoch 18/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0735 - val_loss: 0.0694\n",
      "Epoch 19/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0729 - val_loss: 0.0926\n",
      "Epoch 20/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0735 - val_loss: 0.0709\n",
      "Epoch 21/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0726 - val_loss: 0.0707\n",
      "Epoch 22/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0725 - val_loss: 0.0706\n",
      "Epoch 23/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0723 - val_loss: 0.0686\n",
      "Epoch 24/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0724 - val_loss: 0.0723\n",
      "Epoch 25/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0716 - val_loss: 0.0692\n",
      "Epoch 26/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0721 - val_loss: 0.0700\n",
      "Epoch 27/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0718 - val_loss: 0.0710\n",
      "Epoch 28/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0714 - val_loss: 0.0853\n",
      "Epoch 29/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0714 - val_loss: 0.0690\n",
      "Epoch 30/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0707 - val_loss: 0.0794\n",
      "Epoch 31/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0704 - val_loss: 0.0694\n",
      "Epoch 32/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0712 - val_loss: 0.0684\n",
      "Epoch 33/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0702 - val_loss: 0.0748\n",
      "Epoch 34/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0705 - val_loss: 0.0683\n",
      "Epoch 35/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0706 - val_loss: 0.0696\n",
      "Epoch 36/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0694 - val_loss: 0.0722\n",
      "Epoch 37/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0694 - val_loss: 0.0821\n",
      "Epoch 38/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0698 - val_loss: 0.0695\n",
      "Epoch 39/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0692 - val_loss: 0.0687\n",
      "Epoch 40/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0693 - val_loss: 0.0930\n",
      "Epoch 41/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0693 - val_loss: 0.0717\n",
      "Epoch 42/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0688 - val_loss: 0.0711\n",
      "Epoch 43/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0684 - val_loss: 0.0675\n",
      "Epoch 44/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0681 - val_loss: 0.0814\n",
      "Epoch 45/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0683 - val_loss: 0.0704\n",
      "Epoch 46/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0682 - val_loss: 0.0710\n",
      "Epoch 47/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0674 - val_loss: 0.0678\n",
      "Epoch 48/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0672 - val_loss: 0.0693\n",
      "Epoch 49/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0676 - val_loss: 0.0711\n",
      "Epoch 50/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0678 - val_loss: 0.0719\n",
      "Epoch 51/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0672 - val_loss: 0.0739\n",
      "Epoch 52/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0669 - val_loss: 0.0676\n",
      "Epoch 53/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0661 - val_loss: 0.0721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a56dd02288>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the first model (prediction for 7 days later)\n",
    "model.fit(x2_train, y2_train,\n",
    "          batch_size=50,\n",
    "          epochs=500,\n",
    "          validation_data=(x2_test, y2_test),\n",
    "          callbacks=[early_stopping],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.220788632524059\n"
     ]
    }
   ],
   "source": [
    "print('Temperature prediction for nov. 3.:')\n",
    "print(model.predict(x_input)[0][0]*temp_mean+temp_avg) # predict the output and the normalized data back to a celsius value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2259 samples, validate on 565 samples\n",
      "Epoch 1/500\n",
      "2259/2259 [==============================] - 0s 63us/sample - loss: 0.0980 - val_loss: 0.1017\n",
      "Epoch 2/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0903 - val_loss: 0.1119\n",
      "Epoch 3/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0862 - val_loss: 0.1010\n",
      "Epoch 4/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0852 - val_loss: 0.0975\n",
      "Epoch 5/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0837 - val_loss: 0.0940\n",
      "Epoch 6/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0819 - val_loss: 0.0910\n",
      "Epoch 7/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0819 - val_loss: 0.0897\n",
      "Epoch 8/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0811 - val_loss: 0.0938\n",
      "Epoch 9/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0801 - val_loss: 0.0959\n",
      "Epoch 10/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0807 - val_loss: 0.0891\n",
      "Epoch 11/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0792 - val_loss: 0.0967\n",
      "Epoch 12/500\n",
      "2259/2259 [==============================] - 0s 62us/sample - loss: 0.0795 - val_loss: 0.0953\n",
      "Epoch 13/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0782 - val_loss: 0.0916\n",
      "Epoch 14/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0780 - val_loss: 0.0868\n",
      "Epoch 15/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0771 - val_loss: 0.0861\n",
      "Epoch 16/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0766 - val_loss: 0.0864\n",
      "Epoch 17/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0764 - val_loss: 0.0865\n",
      "Epoch 18/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0764 - val_loss: 0.0896\n",
      "Epoch 19/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0763 - val_loss: 0.0847\n",
      "Epoch 20/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0760 - val_loss: 0.0878\n",
      "Epoch 21/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0750 - val_loss: 0.0897\n",
      "Epoch 22/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0750 - val_loss: 0.0917\n",
      "Epoch 23/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0747 - val_loss: 0.0857\n",
      "Epoch 24/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0740 - val_loss: 0.0963\n",
      "Epoch 25/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0749 - val_loss: 0.0833\n",
      "Epoch 26/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0737 - val_loss: 0.0835\n",
      "Epoch 27/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0736 - val_loss: 0.0840\n",
      "Epoch 28/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0733 - val_loss: 0.0853\n",
      "Epoch 29/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0728 - val_loss: 0.0833\n",
      "Epoch 30/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0730 - val_loss: 0.0851\n",
      "Epoch 31/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0722 - val_loss: 0.0834\n",
      "Epoch 32/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0722 - val_loss: 0.0836\n",
      "Epoch 33/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0716 - val_loss: 0.0861\n",
      "Epoch 34/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0717 - val_loss: 0.0813\n",
      "Epoch 35/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0712 - val_loss: 0.0892\n",
      "Epoch 36/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0714 - val_loss: 0.0829\n",
      "Epoch 37/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0709 - val_loss: 0.0850\n",
      "Epoch 38/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0707 - val_loss: 0.0808\n",
      "Epoch 39/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0712 - val_loss: 0.0905\n",
      "Epoch 40/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0708 - val_loss: 0.0817\n",
      "Epoch 41/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0697 - val_loss: 0.0900\n",
      "Epoch 42/500\n",
      "2259/2259 [==============================] - 0s 58us/sample - loss: 0.0697 - val_loss: 0.0806\n",
      "Epoch 43/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0698 - val_loss: 0.0801\n",
      "Epoch 44/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0691 - val_loss: 0.0918\n",
      "Epoch 45/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0688 - val_loss: 0.0969\n",
      "Epoch 46/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0689 - val_loss: 0.0825\n",
      "Epoch 47/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0683 - val_loss: 0.0794\n",
      "Epoch 48/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0683 - val_loss: 0.0804\n",
      "Epoch 49/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0678 - val_loss: 0.0888\n",
      "Epoch 50/500\n",
      "2259/2259 [==============================] - 0s 60us/sample - loss: 0.0680 - val_loss: 0.0842\n",
      "Epoch 51/500\n",
      "2259/2259 [==============================] - 0s 57us/sample - loss: 0.0676 - val_loss: 0.0815\n",
      "Epoch 52/500\n",
      "2259/2259 [==============================] - 0s 59us/sample - loss: 0.0675 - val_loss: 0.0831\n",
      "Epoch 53/500\n",
      "2259/2259 [==============================] - 0s 61us/sample - loss: 0.0672 - val_loss: 0.0800\n",
      "Epoch 54/500\n",
      "2259/2259 [==============================] - 0s 63us/sample - loss: 0.0669 - val_loss: 0.0933\n",
      "Epoch 55/500\n",
      "2259/2259 [==============================] - 0s 64us/sample - loss: 0.0669 - val_loss: 0.0802\n",
      "Epoch 56/500\n",
      "2259/2259 [==============================] - 0s 65us/sample - loss: 0.0668 - val_loss: 0.0848\n",
      "Epoch 57/500\n",
      "2259/2259 [==============================] - 0s 64us/sample - loss: 0.0664 - val_loss: 0.0817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5f58bca88>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the first model (prediction for 28 days later)\n",
    "model.fit(x3_train, y3_train,\n",
    "          batch_size=50,\n",
    "          epochs=500,\n",
    "          validation_data=(x3_test, y3_test),\n",
    "          callbacks=[early_stopping],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature prediction for nov. 24.:\n",
      "7.169753802539288\n"
     ]
    }
   ],
   "source": [
    "print('Temperature prediction for nov. 24.:')\n",
    "print(model.predict(x_input)[0][0]*temp_mean+temp_avg) # predict the output and the normalized data back to a celsius value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
